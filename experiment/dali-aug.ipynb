{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e29cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "# test_data_root = os.environ[\"DALI_EXTRA_PATH\"]\n",
    "# db_folder = os.path.join(test_data_root, \"db\", \"lmdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "190e9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_transform():\n",
    "    dst_cx, dst_cy = (200, 200)\n",
    "    src_cx, src_cy = (200, 200)\n",
    "\n",
    "    # This function uses homogeneous coordinates - hence, 3x3 matrix\n",
    "\n",
    "    # translate output coordinates to center defined by (dst_cx, dst_cy)\n",
    "    t1 = np.array([[1, 0, -dst_cx], [0, 1, -dst_cy], [0, 0, 1]])\n",
    "\n",
    "    def u():\n",
    "        return np.random.uniform(-0.5, 0.5)\n",
    "\n",
    "    # apply a randomized affine transform - uniform scaling + some random\n",
    "    # distortion\n",
    "    m = np.array([[1 + u(), u(), 0], [u(), 1 + u(), 0], [0, 0, 1]])\n",
    "\n",
    "    # translate input coordinates to center (src_cx, src_cy)\n",
    "    t2 = np.array([[1, 0, src_cx], [0, 1, src_cy], [0, 0, 1]])\n",
    "\n",
    "    # combine the transforms\n",
    "    m = np.matmul(t2, np.matmul(m, t1))\n",
    "\n",
    "    # remove the last row; it's not used by affine transform\n",
    "    return m[0:2, 0:3].astype(np.float32)\n",
    "\n",
    "\n",
    "np.random.seed(seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc3dce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExternalInputCallable(object):\n",
    "    def __call__(self, sample_info):\n",
    "        with open(\"/home/qsvm/lib_ocr/experiment/vietocr_img_441026.jpg\", 'rb') as f:\n",
    "            file_bytes = f.read()\n",
    "        \n",
    "        image = np.frombuffer(file_bytes, dtype=np.uint8)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18931544",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def(seed=12)\n",
    "def example_pipeline():\n",
    "    # This example uses external_source to provide warp matrices\n",
    "    transform = fn.external_source(\n",
    "        batch=False, source=random_transform, dtype=types.FLOAT\n",
    "    )\n",
    "\n",
    "    images = fn.external_source(\n",
    "        source=ExternalInputCallable(),\n",
    "        num_outputs=1,\n",
    "        batch=False,\n",
    "        parallel=True,\n",
    "        dtype=[types.UINT8],\n",
    "        prefetch_queue_depth=2,\n",
    "    )\n",
    "\n",
    "    # The decoder takes tensors containing raw files and outputs images\n",
    "    # as 3D tensors with HWC layout\n",
    "    images = fn.decoders.image(images)\n",
    "\n",
    "    warped_gpu = fn.warp_affine(\n",
    "        images,\n",
    "        transform,  # pass the transform parameters through GPU memory\n",
    "        size=(400, 400),  # specify the output size\n",
    "        # fill_value,       # not specifying `fill_value`\n",
    "        #  results in source coordinate clamping\n",
    "        interp_type=types.INTERP_LINEAR,\n",
    "    )  # use linear interpolation\n",
    "\n",
    "    warped_cpu = fn.warp_affine(\n",
    "        images,\n",
    "        matrix=transform,  # pass the transform through a named input\n",
    "        fill_value=200,\n",
    "        size=(400, 400),  # specify the output size\n",
    "        interp_type=types.INTERP_NN,\n",
    "    )  # use nearest neighbor interpolation\n",
    "\n",
    "    warped_keep_size = fn.warp_affine(\n",
    "        images,\n",
    "        transform,\n",
    "        # size,        # keep the original canvas size\n",
    "        interp_type=types.INTERP_LINEAR,\n",
    "    )  # use linear interpolation\n",
    "    return (\n",
    "        transform,\n",
    "        images,\n",
    "        warped_gpu,\n",
    "        warped_cpu,\n",
    "        warped_keep_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a78016fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when starting Python worker threads for DALI parallel External Source. Cannot fork a process when the CUDA has been initialized in the process. CUDA is initialized during ``Pipeline.build()``, or can be initialized by another library that interacts with CUDA, for example a DL framework creating CUDA tensors. If you are trying to build multiple pipelines that use Python workers, you will need to call ``start_py_workers`` method on all of them before calling ``build`` method of any pipeline to start Python workers before CUDA is initialized by ``build`` or other CUDA operation. Alternatively you can change Python workers starting method from ``fork`` to ``spawn`` (see DALI Pipeline's ``py_start_method`` option for details). ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      2\u001b[0m pipe \u001b[38;5;241m=\u001b[39m example_pipeline(batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, device_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/pipeline.py:1080\u001b[0m, in \u001b[0;36mPipeline.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1077\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline created with `num_threads` < 1 can only be used \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor serialization.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1078\u001b[0m     )\n\u001b[0;32m-> 1080\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_py_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_prepared:\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pipeline_backend()\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/pipeline.py:1033\u001b[0m, in \u001b[0;36mPipeline.start_py_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_graph()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py_pool_started:\n\u001b[0;32m-> 1033\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_py_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/pipeline.py:883\u001b[0m, in \u001b[0;36mPipeline._start_py_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_input_callbacks:\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py_pool \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerPool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_groups\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_input_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prefetch_queue_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_py_start_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_py_num_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpy_callback_pickler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_py_callback_pickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;66;03m# ensure processes started by the pool are terminated when pipeline is no longer used\u001b[39;00m\n\u001b[1;32m    892\u001b[0m weakref\u001b[38;5;241m.\u001b[39mfinalize(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m pool: pool\u001b[38;5;241m.\u001b[39mclose(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py_pool)\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/_multiproc/pool.py:733\u001b[0m, in \u001b[0;36mWorkerPool.from_groups\u001b[0;34m(cls, groups, keep_alive_queue_size, batch_size, start_method, num_workers, min_initial_chunk_size, py_callback_pickler)\u001b[0m\n\u001b[1;32m    731\u001b[0m pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 733\u001b[0m     pool \u001b[38;5;241m=\u001b[39m \u001b[43mProcPool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpy_callback_pickler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# close underlying file descriptors that are not needed anymore once\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# passed to the workers processes\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m context \u001b[38;5;129;01min\u001b[39;00m contexts:\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/_multiproc/pool.py:426\u001b[0m, in \u001b[0;36mProcPool.from_contexts\u001b[0;34m(cls, contexts, num_workers, start_method, py_callback_pickler)\u001b[0m\n\u001b[1;32m    424\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 426\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneral_task_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_pickler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m general_task_queue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         general_task_queue\u001b[38;5;241m.\u001b[39mclose_handle()\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/_multiproc/pool.py:347\u001b[0m, in \u001b[0;36mProcPool.__init__\u001b[0;34m(self, mp, workers_contexts, result_queue, general_task_queue, callback_pickler)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot start a pool with no workers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfork\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _b\u001b[38;5;241m.\u001b[39mIsDriverInitialized():\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    348\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when starting Python worker threads for DALI parallel External Source. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot fork a process when the CUDA has been initialized in the process. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA is initialized during ``Pipeline.build()``, or can be initialized by another\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m library that interacts with CUDA, for example a DL framework creating \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA tensors. If you are trying to build multiple pipelines that use Python \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkers, you will need to call ``start_py_workers`` method on all of them before \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling ``build`` method of any pipeline to start Python workers before CUDA is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitialized by ``build`` or other CUDA operation. Alternatively you can change \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython workers starting method from ``fork`` to ``spawn`` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(see DALI Pipeline\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms ``py_start_method`` option for details). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers_contexts \u001b[38;5;241m=\u001b[39m workers_contexts\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_queue \u001b[38;5;241m=\u001b[39m result_queue\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error when starting Python worker threads for DALI parallel External Source. Cannot fork a process when the CUDA has been initialized in the process. CUDA is initialized during ``Pipeline.build()``, or can be initialized by another library that interacts with CUDA, for example a DL framework creating CUDA tensors. If you are trying to build multiple pipelines that use Python workers, you will need to call ``start_py_workers`` method on all of them before calling ``build`` method of any pipeline to start Python workers before CUDA is initialized by ``build`` or other CUDA operation. Alternatively you can change Python workers starting method from ``fork`` to ``spawn`` (see DALI Pipeline's ``py_start_method`` option for details). "
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "pipe = example_pipeline(batch_size=batch_size, num_threads=2, device_id=0)\n",
    "pipe.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "636ca52b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when starting Python worker threads for DALI parallel External Source. Cannot fork a process when the CUDA has been initialized in the process. CUDA is initialized during ``Pipeline.build()``, or can be initialized by another library that interacts with CUDA, for example a DL framework creating CUDA tensors. If you are trying to build multiple pipelines that use Python workers, you will need to call ``start_py_workers`` method on all of them before calling ``build`` method of any pipeline to start Python workers before CUDA is initialized by ``build`` or other CUDA operation. Alternatively you can change Python workers starting method from ``fork`` to ``spawn`` (see DALI Pipeline's ``py_start_method`` option for details). ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe_out \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/pipeline.py:1414\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, cuda_stream, **pipeline_inputs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pipeline_inputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_are_pipeline_inputs_possible():\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;124m        When using pipeline_inputs named arguments, either\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;124m        operator.)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1413\u001b[0m     )\n\u001b[0;32m-> 1414\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inp_name, inp_value \u001b[38;5;129;01min\u001b[39;00m pipeline_inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_input(inp_name, inp_value)\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/pipeline.py:1080\u001b[0m, in \u001b[0;36mPipeline.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1077\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline created with `num_threads` < 1 can only be used \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor serialization.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1078\u001b[0m     )\n\u001b[0;32m-> 1080\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_py_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_prepared:\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pipeline_backend()\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/pipeline.py:1033\u001b[0m, in \u001b[0;36mPipeline.start_py_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_graph()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py_pool_started:\n\u001b[0;32m-> 1033\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_py_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/pipeline.py:883\u001b[0m, in \u001b[0;36mPipeline._start_py_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parallel_input_callbacks:\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py_pool \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerPool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_groups\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_input_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prefetch_queue_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_py_start_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_py_num_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpy_callback_pickler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_py_callback_pickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;66;03m# ensure processes started by the pool are terminated when pipeline is no longer used\u001b[39;00m\n\u001b[1;32m    892\u001b[0m weakref\u001b[38;5;241m.\u001b[39mfinalize(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m pool: pool\u001b[38;5;241m.\u001b[39mclose(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py_pool)\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/_multiproc/pool.py:733\u001b[0m, in \u001b[0;36mWorkerPool.from_groups\u001b[0;34m(cls, groups, keep_alive_queue_size, batch_size, start_method, num_workers, min_initial_chunk_size, py_callback_pickler)\u001b[0m\n\u001b[1;32m    731\u001b[0m pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 733\u001b[0m     pool \u001b[38;5;241m=\u001b[39m \u001b[43mProcPool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpy_callback_pickler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# close underlying file descriptors that are not needed anymore once\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# passed to the workers processes\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m context \u001b[38;5;129;01min\u001b[39;00m contexts:\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/_multiproc/pool.py:426\u001b[0m, in \u001b[0;36mProcPool.from_contexts\u001b[0;34m(cls, contexts, num_workers, start_method, py_callback_pickler)\u001b[0m\n\u001b[1;32m    424\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 426\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneral_task_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_pickler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m general_task_queue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         general_task_queue\u001b[38;5;241m.\u001b[39mclose_handle()\n",
      "File \u001b[0;32m~/miniconda3/envs/ocr/lib/python3.11/site-packages/nvidia/dali/_multiproc/pool.py:347\u001b[0m, in \u001b[0;36mProcPool.__init__\u001b[0;34m(self, mp, workers_contexts, result_queue, general_task_queue, callback_pickler)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot start a pool with no workers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfork\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _b\u001b[38;5;241m.\u001b[39mIsDriverInitialized():\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    348\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when starting Python worker threads for DALI parallel External Source. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot fork a process when the CUDA has been initialized in the process. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA is initialized during ``Pipeline.build()``, or can be initialized by another\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m library that interacts with CUDA, for example a DL framework creating \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA tensors. If you are trying to build multiple pipelines that use Python \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkers, you will need to call ``start_py_workers`` method on all of them before \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling ``build`` method of any pipeline to start Python workers before CUDA is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitialized by ``build`` or other CUDA operation. Alternatively you can change \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython workers starting method from ``fork`` to ``spawn`` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(see DALI Pipeline\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms ``py_start_method`` option for details). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers_contexts \u001b[38;5;241m=\u001b[39m workers_contexts\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_queue \u001b[38;5;241m=\u001b[39m result_queue\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error when starting Python worker threads for DALI parallel External Source. Cannot fork a process when the CUDA has been initialized in the process. CUDA is initialized during ``Pipeline.build()``, or can be initialized by another library that interacts with CUDA, for example a DL framework creating CUDA tensors. If you are trying to build multiple pipelines that use Python workers, you will need to call ``start_py_workers`` method on all of them before calling ``build`` method of any pipeline to start Python workers before CUDA is initialized by ``build`` or other CUDA operation. Alternatively you can change Python workers starting method from ``fork`` to ``spawn`` (see DALI Pipeline's ``py_start_method`` option for details). "
     ]
    }
   ],
   "source": [
    "pipe_out = pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0  # change this value to see other images from the batch;\n",
    "# it must be in 0..batch_size-1 range\n",
    "\n",
    "# from synsets import imagenet_synsets\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "len_outputs = len(pipe_out) - 2\n",
    "\n",
    "captions = [\n",
    "    \"original\",\n",
    "    \"warp GPU (linear, border clamp)\",\n",
    "    \"warp CPU (nearest, fill)\",\n",
    "    \"warp GPU (keep canvas size)\",\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "# plt.suptitle(imagenet_synsets[pipe_out[0].at(n)[0]], fontsize=16)\n",
    "columns = 2\n",
    "rows = int(math.ceil(len_outputs / columns))\n",
    "gs = gridspec.GridSpec(rows, columns)\n",
    "\n",
    "print(\"Affine transform matrix:\")\n",
    "print(pipe_out[1].at(n))\n",
    "\n",
    "for i in range(len_outputs):\n",
    "    plt.subplot(gs[i])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(captions[i])\n",
    "    pipe_out_cpu = pipe_out[2 + i].as_cpu()\n",
    "    img_chw = pipe_out_cpu.at(n)\n",
    "    plt.imshow((img_chw) / 255.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
